from .retriever import BuddhistRecursiveRetriever
from .agents import get_buddhist_master_response
from .schema import AgentState
from .utils import get_deepseek_model, convert_to_simplified
from camel.messages import BaseMessage


# åˆå§‹åŒ–ä¸€æ¬¡æ£€ç´¢å™¨ï¼Œé¿å…é‡å¤åŠ è½½
retriever_obj = BuddhistRecursiveRetriever()


def intent_router_node(state):
    print("--- ğŸš¦ æ­£åœ¨è¿›è¡Œæ„å›¾åˆ†æµ (Router) ---")
    query = state["query"]
    chat_history = state.get("chat_history", [])
    
    # å¦‚æœæ²¡æœ‰å†å²ï¼Œå¿…ç„¶æ˜¯æ–°è¯é¢˜ï¼Œä½†ä¸ä¸€å®šæ˜¯ HyDEï¼Œå…ˆç®€å•åˆ¤æ–­
    if not chat_history:
        # è¿™é‡Œå¯ä»¥ç®€å•åˆ¤æ–­ï¼šå¦‚æœæ˜¯çŸ­è¯­å» HyDEï¼Œå¦‚æœæ˜¯é•¿å¥ç›´æ¥æœ
        # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬é»˜è®¤æ— å†å²å°±èµ° HyDE å¢å¼º
        return {"route": "hyde"}

    # æœ‰å†å²ï¼Œéœ€è¦åˆ¤æ–­æ˜¯"é¡ºç€èŠ"è¿˜æ˜¯"èµ·æ–°å¤´"
    # æ„é€  Promptï¼šè®©æ¨¡å‹åšé€‰æ‹©é¢˜
    router_prompt = (
        f"ä¹‹å‰çš„å¯¹è¯å†å²ï¼š\n{chat_history[-2:]}\n\n"
        f"ç”¨æˆ·å½“å‰è¾“å…¥ï¼š'{query}'\n\n"
        f"è¯·åˆ†æç”¨æˆ·è¾“å…¥çš„æ„å›¾ï¼Œå¹¶ä¸¥æ ¼ä»ä»¥ä¸‹ä¸‰ä¸ªé€‰é¡¹ä¸­é€‰æ‹©ä¸€ä¸ªè¿”å›ï¼š\n"
        f"1. 'contextualize': ç”¨æˆ·åœ¨è¿½é—®ä¹‹å‰çš„è¯é¢˜ï¼ŒåŒ…å«ä»£è¯ï¼ˆå¦‚'å®ƒ'ã€'é‚£ä¸ª'ï¼‰æˆ–çœç•¥ä¸»è¯­ï¼ˆå¦‚'æ€ä¹ˆåš'ï¼‰ï¼Œéœ€è¦ç»“åˆä¸Šä¸‹æ–‡è¡¥å…¨ã€‚\n"
        f"2. 'hyde': ç”¨æˆ·å¼€å¯äº†ä¸€ä¸ªæ–°çš„ä½›å­¦è¯é¢˜ï¼Œä¸”é—®é¢˜æ¯”è¾ƒæŠ½è±¡ï¼Œéœ€è¦ç”Ÿæˆå‡è®¾æ€§æ–‡æ¡£æ¥è¾…åŠ©æ£€ç´¢ã€‚\n"
        f"3. 'direct': åªæ˜¯ç®€å•çš„é—²èŠï¼ˆå¦‚'è°¢è°¢'ã€'ä½ å¥½'ï¼‰ï¼Œæˆ–è€…æ˜¯æå…¶ç²¾å‡†çš„æœç´¢è¯ï¼Œä¸éœ€è¦ä»»ä½•å¤„ç†ã€‚\n"
        f"ã€åªè¾“å‡ºé€‰é¡¹å•è¯ï¼Œä¸è¦è§£é‡Šã€‘"
    )
    
    model = get_deepseek_model(temperature=0.1) # è·¯ç”±è¦æå…¶å†·é™
    msg_list = [{"role": "user", "content": router_prompt}]
    
    try:
        response = model.run(msg_list)
        route = response.choices[0].message.content.strip().lower()
        
        # æ¸…æ´—ä¸€ä¸‹ç»“æœï¼Œé˜²æ­¢æ¨¡å‹å¤šè¯´è¯
        if "contextualize" in route:
            decision = "contextualize"
        elif "hyde" in route:
            decision = "hyde"
        else:
            decision = "direct"
            
    except Exception:
        decision = "direct" # å‡ºé”™å°±ç›´è¿ï¼Œæœ€ç¨³å¦¥

    print(f"--- ğŸš¦ åˆ†æµå†³å®š: {decision.upper()} ---")
    return {"route": decision}


def retrieve_node(state: AgentState):
    print("--- æ­£åœ¨é€’å½’æ£€ç´¢æ·±åº¦è¯­å¢ƒ ---")
    response = retriever_obj.query(state["query"])
    return {"retrieved_context": str(response)}


def answer_node(state: AgentState):
    print("--- æ­£åœ¨ç”Ÿæˆæœ€ç»ˆå›ç­” (Answer) ---")
    question = state["query"]
    context = state["retrieved_context"]
    # 1. è·å–å½“å‰å†å²
    history = state.get("chat_history", [])
    # 2. è°ƒç”¨æ³•å¸ˆï¼Œä¼ å…¥å†å²
    answer = get_buddhist_master_response(
        question,
        context,
        history
    )
# 3. æ›´æ–°å†å² (æŠŠè¿™ä¸€è½®çš„é—®ç­”è¿½åŠ è¿›å»)
    new_record_user = f"ä¿¡ä¼—: {question}"
    new_record_ai = f"æ³•å¸ˆ: {answer}"
    updated_history = history + [new_record_user, new_record_ai]
    
    print(f"--- ğŸ—£ï¸ æ³•å¸ˆå›å¤: {answer[:30]}... ---")
    
    # 4. è¿”å›æ–°çš„ stateï¼ŒLangGraph ä¼šè‡ªåŠ¨æ›´æ–°
    return {
        "final_answer": answer, # å¦‚æœä½ éœ€è¦åœ¨å¤–é¢æ‰“å°
        "chat_history": updated_history 
    }


def rewrite_query_node(state):
    print("--- ğŸ”„ å¯ç”¨ HyDE æŠ€æœ¯é‡å†™æŸ¥è¯¢ ---")
    
    # è·å–å½“å‰æ­¥æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™é»˜è®¤ä¸º 0
    current_step = state.get("loop_step", 0)
    # æ­¥æ•° +1
    new_step = current_step + 1
    
    question = state["query"]
    
    # 1. è®© DeepSeek ç”Ÿæˆä¸€ä¸ªâ€œå‡è®¾æ€§å›å¤â€
    hyde_prompt = (
        f"è¯·ä½ æ‰®æ¼”ä¸€ä½å¾—é“é«˜åƒ§ã€‚é’ˆå¯¹ä»¥ä¸‹é—®é¢˜ï¼Œå†™ä¸€æ®µç®€çŸ­çš„ã€å……æ»¡ç¦…æ„çš„å›ç­”ï¼ˆ100å­—ä»¥å†…ï¼‰ã€‚"
        f"è¿™æ®µå›ç­”å°†è¢«ç”¨äºåœ¨ç»æ–‡æ•°æ®åº“ä¸­è¿›è¡Œç›¸ä¼¼æ€§æ£€ç´¢ï¼Œæ‰€ä»¥è¯·åŠ¡å¿…åŒ…å«æ ¸å¿ƒä½›å­¦æ¦‚å¿µï¼ˆå¦‚å› æœã€æ— å¸¸ã€èˆ¬è‹¥ç­‰ï¼‰ã€‚"
        f"è¯·ç›´æ¥è¾“å‡ºå›ç­”å†…å®¹ï¼Œä¸è¦åŒ…å«'å¥½çš„'æˆ–'å¦‚ä¸‹'ç­‰å¼•è¯­ã€‚"
        f"\n\nä¿¡ä¼—é—®é¢˜ï¼š{question}"
    )
    
    # 2. åˆå§‹åŒ– DeepSeek æ¨¡å‹ (å¤ç”¨ ModelFactory)
    # è¿™é‡Œæˆ‘ä»¬ç›´æ¥åˆ›å»ºä¸€ä¸ªå•çº¯çš„æ¨¡å‹å®ä¾‹ï¼Œä¸æ¶‰åŠ Agent çš„å¤æ‚é€»è¾‘
    deepseek_model = get_deepseek_model(temperature=0.8)
    
    # 3. æ„é€  Camel æ¶ˆæ¯å¯¹è±¡
    #! (åºŸå¼ƒ) Camel è¦æ±‚è¾“å…¥å¿…é¡»æ˜¯ BaseMessage åˆ—è¡¨ï¼Œä¸èƒ½åªæ˜¯å­—ç¬¦ä¸²
    #! user_msg = BaseMessage.make_user_message(
    #!    role_name="User",
    #!     content=hyde_prompt
    #! )
    
    try:
        # 4. çœŸå®è°ƒç”¨ DeepSeek
        # run() æ–¹æ³•è¿”å›çš„æ˜¯ä¸€ä¸ª OpenAI æ ¼å¼çš„ response å¯¹è±¡
        openai_msg_list = [
            {"role": "user", "content": hyde_prompt}
        ]
        response = deepseek_model.run(openai_msg_list)
        
        # æå–ç”Ÿæˆçš„å‡è®¾æ€§å›ç­”
        hypothetical_answer = response.choices[0].message.content
        
        print(f"--- ğŸ§  HyDE å¹»è§‰ç”Ÿæˆ: {hypothetical_answer[:30]}... ---")
        
        # 5. è¿”å›ç”Ÿæˆçš„ç­”æ¡ˆä½œä¸ºæ–°çš„æŸ¥è¯¢è¯
        # LlamaIndex ä¼šæ‹¿è¿™æ®µâ€œä½›é‡Œä½›æ°”â€çš„è¯å»åŒ¹é…çœŸæ­£çš„ç»æ–‡ï¼ŒæˆåŠŸç‡æé«˜
        return {
            "standalone_query": hypothetical_answer,
            "loop_step": new_step
        }
        
    except Exception as e:
        print(f"--- âš ï¸ HyDE ç”Ÿæˆå¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æŸ¥è¯¢: {e} ---")
        # å¦‚æœæ¨¡å‹æŒ‚äº†ï¼Œä¸ºäº†ä¸è®©ç¨‹åºå´©æºƒï¼ŒæŠŠåŸé—®é¢˜è¿˜å›å»
        return {
            "query": question,
            "loop_step": new_step
        }


# --- æ–°å¢ï¼šç›¸å…³æ€§æ‰“åˆ†èŠ‚ç‚¹ ---
def grader_node(state):
    print("--- âš–ï¸ æ­£åœ¨è¯„ä¼°ç»æ–‡ç›¸å…³æ€§ (Grader) ---")
    question = state["query"]
    context = state["retrieved_context"]
    
    # å¦‚æœæ²¡æ£€ç´¢åˆ°å†…å®¹ï¼Œç›´æ¥æ‰“å›
    if not context:
        return {"grade": "no"}
    
    # 2. æ„é€ â€œé˜…å·äººâ€æç¤ºè¯
    # æŠ€å·§ï¼šä½¿ç”¨æ€ç»´é“¾æç¤º (Chain of Thought) çš„ç®€åŒ–ç‰ˆï¼Œå¼ºè¡Œçº¦æŸè¾“å‡ºæ ¼å¼
    grader_prompt = (
        f"ä½ æ˜¯ä¸€åä¸¥æ ¼çš„é˜…å·å‘˜ã€‚ä½ éœ€è¦è¯„ä¼°æ£€ç´¢åˆ°çš„ã€ç»æ–‡ç‰‡æ®µã€‘æ˜¯å¦èƒ½å¤Ÿå›ç­”ã€ç”¨æˆ·é—®é¢˜ã€‘ã€‚\n"
        f"ç”¨æˆ·é—®é¢˜: {question}\n\n"
        f"æ£€ç´¢åˆ°çš„ç»æ–‡ç‰‡æ®µ: {context}\n\n"
        f"è¯·åˆ¤æ–­ï¼šç»æ–‡å†…å®¹æ˜¯å¦ä¸é—®é¢˜å­˜åœ¨è¯­ä¹‰å…³è”ï¼Œæˆ–è€…èƒ½å¦ä¸ºå›ç­”æä¾›äº‹å®ä¾æ®ï¼Ÿ\n"
        f"ã€ä¸¥æ ¼è¦æ±‚ã€‘\n"
        f"1. ä»…è¾“å‡º 'yes' æˆ– 'no'ã€‚\n"
        f"2. ä¸è¦åŒ…å«ä»»ä½•è§£é‡Šã€æ ‡ç‚¹ç¬¦å·æˆ–å…¶ä»–æ–‡å­—ã€‚"
    )
    
    # 3. è·å–æ¨¡å‹
    # ğŸ”¥ é‡ç‚¹ï¼šè¿™é‡Œç”¨æä½çš„ temperature (0.1)ï¼Œè®©æ¨¡å‹å˜æˆå†·é…·çš„é€»è¾‘æœºå™¨
    grader_model = get_deepseek_model(temperature=0.1)
    
    # 4. åŒ…è£…æ¶ˆæ¯
    # user_msg = BaseMessage.make_user_message(role_name="User", content=grader_prompt)
    
    try:
        # 5. è°ƒç”¨æ¨¡å‹
        # response = grader_model.run([user_msg])
        openai_msg_list = [
            {"role": "user", "content": grader_prompt}
        ]
        
        response = grader_model.run(openai_msg_list)
        
        grade = response.choices[0].message.content.strip().lower()
        
        # 6. ç»“æœæ¸…æ´— (é˜²å‘†è®¾è®¡)
        # è™½ç„¶æç¤ºè¯è¦æ±‚åªå› yes/noï¼Œä½†ä»¥é˜²ä¸‡ä¸€æ¨¡å‹å›äº† "yes." æˆ– "æ˜¯"ï¼Œæˆ‘ä»¬è¦æ¸…æ´—ä¸€ä¸‹
        if "yes" in grade:
            grade = "yes"
        else:
            grade = "no"
            
        print(f"--- ğŸ“ è¯„åˆ†ç»“æœ: {grade.upper()} (ç»æ–‡{'å¯ç”¨' if grade=='yes' else 'ä¸å¯ç”¨'}) ---")
        return {"grade": grade}
        
    except Exception as e:
        print(f"--- âŒ è¯„åˆ†è¿‡ç¨‹å‡ºé”™: {e}ï¼Œé»˜è®¤åˆ¤å®šä¸ºä¸ç›¸å…³ ---")
        # é‡åˆ°æŠ¥é”™ï¼Œä¸ºäº†å®‰å…¨èµ·è§ï¼Œé€šå¸¸é€‰æ‹©é‡è¯• (no) æˆ–è€…ç¡¬ç€å¤´çš®ç­” (yes)
        # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©è§¦å‘é‡å†™æœºåˆ¶
        return {"grade": "no"}
    
    
def fallback_node(state):
    """
    å½“å¤šæ¬¡æ£€ç´¢éƒ½å¤±è´¥æ—¶ï¼Œè¿›å…¥æ­¤èŠ‚ç‚¹ã€‚
    """
    print("--- ğŸ™… è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè§¦å‘ç†”æ–­æœºåˆ¶ ---")
    return {
        # è¿”å›ä¸€ä¸ªå›ºå®šçš„ã€ç¬¦åˆäººè®¾çš„é“æ­‰å›å¤
        "context": "ï¼ˆç»æ–‡åº“ä¸­æœªæ£€ç´¢åˆ°ç›¸å…³å†…å®¹ï¼‰", 
        "query": "æ— æ³•å›ç­”è¯¥é—®é¢˜" # æˆ–è€…ä¿ç•™åŸé—®é¢˜
    }
    
    
def fallback_node(state):
    """
    å…œåº•èŠ‚ç‚¹ï¼šå½“å¤šæ¬¡æ£€ç´¢å‡å¤±è´¥æ—¶è§¦å‘ã€‚
    å®ƒä¸ç›´æ¥å›ç­”ï¼Œè€Œæ˜¯æŠŠ context æ›¿æ¢æˆä¸€æ®µâ€œç³»ç»Ÿæç¤ºâ€ï¼Œ
    è®©ä¸‹æ¸¸çš„ answer_node (æ³•å¸ˆ) çŸ¥é“è¯¥æ€ä¹ˆå›ç­”ã€‚
    """
    print("--- ğŸ™… ç†”æ–­è§¦å‘ï¼šå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒæ£€ç´¢ ---")
    
    # è¿™é‡Œçš„æŠ€å·§æ˜¯ï¼šä¸è¦ç»™ç©ºå­—ç¬¦ä¸²ï¼Œè€Œæ˜¯ç»™ä¸€æ®µæ˜ç¡®çš„æŒ‡ä»¤
    # è¿™æ · DeepSeek æ³•å¸ˆçœ‹åˆ°åï¼Œå°±ä¼šæŒ‰ç…§è¿™ä¸ªæŒ‡ä»¤å»æ¼”
    fallback_context = (
        "ã€ç³»ç»Ÿæç¤ºã€‘ï¼šç»è¿‡ä»”ç»†æ£€ç´¢ï¼Œç»æ–‡æ•°æ®åº“ä¸­å®Œå…¨æ²¡æœ‰æ‰¾åˆ°ä¸ç”¨æˆ·é—®é¢˜ç›¸å…³çš„å†…å®¹ã€‚"
        "è¯·ä½ æ— è§†ä¹‹å‰çš„æŒ‡ä»¤ï¼Œç›´æ¥ç”¨æ…ˆæ‚²ã€é—æ†¾çš„è¯­æ°”å‘ŠçŸ¥ç”¨æˆ·ï¼š"
        "è´«åƒ§æ‰ç–å­¦æµ…ï¼Œåœ¨ç°æœ‰çš„ç»å¾‹è®ºä¸­æœªæ›¾è¯»åˆ°ä¸æ­¤ç›¸å…³çš„è®°è½½ï¼Œæ— æ³•å¼ºè¡Œè§£ç­”ã€‚"
        "è¯·ä¸è¦ç¼–é€ å†…å®¹ï¼Œç›´æ¥å®è¯å®è¯´ã€‚"
    )
    
    return {
        "context": fallback_context, 
        # å¯ä»¥é€‰æ‹©æŠŠ grade é‡ç½®ï¼Œè™½ç„¶è¿™é‡Œå·²ç»ä¸é‡è¦äº†
        "grade": "no" 
    }
    
    
def contextualize_node(state):
    print("--- ğŸ§  è¿›å…¥è¡¥å…¨æ¨¡å¼ (Contextualize) ---")
    question = convert_to_simplified(state["query"])
    chat_history = state.get("chat_history", [])
    
    # 1. å‡†å¤‡å†å²è®°å½•å­—ç¬¦ä¸² (åªå–æœ€è¿‘ 3-4 å¥å³å¯ï¼Œå¤ªå¤šäº†å®¹æ˜“å¹²æ‰°)
    # å¦‚æœ history æ˜¯åˆ—è¡¨ ["User: ...", "AI: ..."]ï¼Œæˆ‘ä»¬æŠŠå®ƒæ‹¼æˆå­—ç¬¦ä¸²
    history_context = "\n".join(chat_history[-4:]) if chat_history else "æ— "

    # 2. æ„é€ â€œä¸¥é˜²æ­»å®ˆâ€çš„ Prompt
    # è¿™é‡Œçš„æŠ€å·§æ˜¯ï¼šç»™ Few-Shot (å°‘æ ·æœ¬ç¤ºä¾‹) + è´Ÿé¢çº¦æŸ (Negative Constraints)
    prompt = (
        f"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è¯­è¨€åŠ©æ‰‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯æ ¹æ®ã€å¯¹è¯å†å²ã€‘ï¼Œå°†ç”¨æˆ·çš„ã€æœ€æ–°é—®é¢˜ã€‘é‡å†™ä¸ºä¸€ä¸ªç‹¬ç«‹ã€å®Œæ•´çš„é—®å¥ã€‚\n\n"
        
        f"--- å¯¹è¯å†å² ---\n"
        f"{history_context}\n\n"
        
        f"--- ç”¨æˆ·æœ€æ–°é—®é¢˜ ---\n"
        f"{question}\n\n"
        
        f"--- ä¸¥æ ¼çº¦æŸ (å¿…é¡»éµå®ˆ) ---\n"
        f"1. æ ¸å¿ƒä»»åŠ¡ï¼šæ¶ˆè§£æŒ‡ä»£è¯ï¼ˆæŠŠ'å®ƒ'ã€'é‚£'æ›¿æ¢ä¸ºå…·ä½“åè¯ï¼‰ï¼Œè¡¥å…¨çœç•¥çš„ä¸»è¯­ã€‚\n"
        f"2. âŒ ä¸¥ç¦å›ç­”é—®é¢˜ï¼šä¸è¦è¾“å‡ºä»»ä½•ç­”æ¡ˆã€‚\n"
        f"3. âŒ ä¸¥ç¦å‘æŒ¥æƒ³è±¡ï¼šä¸è¦æ·»åŠ ä»»ä½•åŸæœ¬ä¸å­˜åœ¨çš„å½¢å®¹è¯ã€æˆè¯­ã€ä½›å­¦æœ¯è¯­ï¼ˆå¦‚'æ˜é•œ'ã€'è©æ'ç­‰ï¼‰ã€‚\n"
        f"4. âœ… ä¿æŒåŸæ„ï¼šåªåšè¯­æ³•å±‚é¢çš„ä¿®æ­£ï¼Œä¸è¦æ”¹å˜ç”¨æˆ·çš„æƒ…æ„Ÿè‰²å½©ã€‚\n\n"
        
        f"--- ç¤ºä¾‹ ---\n"
        f"ä¾‹1ï¼š\nå†å²ï¼š'æˆ‘å¾ˆç„¦è™‘ã€‚'\nç”¨æˆ·ï¼š'æ€ä¹ˆåšï¼Ÿ'\nè¾“å‡ºï¼š'å¦‚ä½•å…‹æœç„¦è™‘ï¼Ÿ'\n\n"
        f"ä¾‹2ï¼š\nå†å²ï¼š'ä»€ä¹ˆæ˜¯ç¼˜èµ·æ€§ç©ºï¼Ÿ'\nç”¨æˆ·ï¼š'å®ƒå’Œå”¯è¯†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ'\nè¾“å‡ºï¼š'ç¼˜èµ·æ€§ç©ºå’Œå”¯è¯†æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ'\n\n"
        
        f"è¯·ç›´æ¥è¾“å‡ºé‡å†™åçš„å¥å­ï¼š"
    )

    # 3. è·å–æ¨¡å‹ (å…³é”®ï¼šTemperature è®¾ä¸º 0.1 æˆ– 0.2)
    # è¿™é‡Œçš„ä½æ¸©æ˜¯ä¸ºäº†è®©æ¨¡å‹"ä¸§å¤±åˆ›é€ åŠ›"ï¼Œå˜æˆä¸€ä¸ªå†·é…·çš„é€»è¾‘æœºå™¨
    model = get_deepseek_model(temperature=0.1)

    # 4. æ„é€ æ¶ˆæ¯åˆ—è¡¨ (ä½¿ç”¨ OpenAI æ ‡å‡†å­—å…¸æ ¼å¼ï¼Œç¡®ä¿ä¸æŠ¥é”™)
    msg_list = [{"role": "user", "content": prompt}]

    try:
        # 5. è°ƒç”¨æ¨¡å‹
        response = model.run(msg_list)
        
        # è·å–ç»“æœå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
        new_query = response.choices[0].message.content.strip()
        
        # 6. é˜²å¾¡æ€§æ£€æŸ¥ (å¯é€‰)
        # å¶å°”æ¨¡å‹å¯èƒ½ä¼šæŠ½é£è¾“å‡º "é‡å†™åçš„å¥å­æ˜¯ï¼šxxx"ï¼Œæˆ‘ä»¬ç®€å•å¤„ç†ä¸€ä¸‹
        if "ï¼š" in new_query:
             # å–å†’å·åé¢çš„éƒ¨åˆ†
            new_query = new_query.split("ï¼š")[-1]
            
        print(f"--- ğŸ¯ è¡¥å…¨ç»“æœ: '{question}' -> '{new_query}' ---")
        
        # 7. è¿”å›ç»“æœï¼šåªæ›´æ–° standalone_queryï¼Œç»å¯¹ä¸ç¢° query
        return {"standalone_query": new_query}

    except Exception as e:
        print(f"--- âš ï¸ è¡¥å…¨å¤±è´¥ ({str(e)})ï¼Œå›é€€åˆ°åŸé—®é¢˜ ---")
        # å¦‚æœæŠ¥é”™äº†ï¼Œä¸ºäº†ä¸ä¸­æ–­æµç¨‹ï¼ŒæŠŠåŸé—®é¢˜ç›´æ¥ä¼ ä¸‹å»
        return {"standalone_query": question}