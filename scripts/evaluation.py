import os
import sys
import pandas as pd
import warnings
from datasets import Dataset 
from openai import OpenAI
import nest_asyncio

# --- Ragas 0.4.3 æ ¸å¿ƒç»„ä»¶å¯¼å…¥ ---
from ragas import evaluate
from ragas.metrics import (
    Faithfulness,      # å¿ å®åº¦ï¼šæ˜¯å¦èƒ¡è¯´å…«é“
    AnswerRelevancy,   # ç›¸å…³æ€§ï¼šæ˜¯å¦ç­”éæ‰€é—®
    ContextPrecision,  # æ£€ç´¢ç²¾åº¦ï¼šæŸ¥åˆ°çš„ç»æ–‡å«é‡‘é‡
    ContextRecall      # æ£€ç´¢å¬å›ï¼šæ˜¯å¦æ¼äº†å…³é”®ç»æ–‡
)
from ragas.llms import llm_factory
# from ragas.embeddings import HuggingFaceEmbeddings
from ragas.run_config import RunConfig

# LangChain ç»„ä»¶ (ç”¨äºæ„å»ºæ›¿ä»£ç‰ˆæ£€ç´¢å™¨)
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings as LangChainHFEmbeddings

# è§£å†³å¼‚æ­¥åµŒå¥—å’Œè­¦å‘Š
nest_asyncio.apply()
warnings.filterwarnings("ignore", category=UserWarning, module="ragas")

# --- è·¯å¾„é…ç½® ---
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from src.config import PERSIST_PATH, DEVICE
from src.retriever import BuddhistRecursiveRetriever
from src.agents import get_buddhist_master_response 

# é…ç½®è¾“å…¥è¾“å‡ºè·¯å¾„
TESTSET_PATH = "./testdata/dharma_db_testset.csv" # ä½¿ç”¨æˆ‘ä»¬åˆšæ‰ç”Ÿæˆçš„ä¸­æ–‡æµ‹è¯•é›†
OUTPUT_REPORT = "./testdata/evaluation_report.csv"
# ==============================================================================
# ğŸ› ï¸ ä¸´æ—¶è¡¥ä¸ï¼šå®šä¹‰ä¸€ä¸ªç›´è¿ Chroma çš„æ£€ç´¢å™¨
# ==============================================================================
class DirectChromaRetriever:
    def __init__(self, persist_path, collection_name="buddhist_sutras"):
        print(f"--- ğŸ› ï¸ æ­£åœ¨åˆå§‹åŒ–ç›´è¿æ£€ç´¢å™¨ (Bypass docstore.json) ---")
        print(f"--- æ•°æ®åº“è·¯å¾„: {persist_path} ---")
        
        # 1. åˆå§‹åŒ– Embedding (å¿…é¡»å’Œå…¥åº“æ—¶ç”¨çš„å®Œå…¨ä¸€è‡´ï¼)
        # å¦‚æœä½ å…¥åº“æ—¶ç”¨çš„æ˜¯ BAAI/bge-small-zh-v1.5ï¼Œè¿™é‡Œå¿…é¡»ä¸€æ ·
        self.embedding_func = LangChainHFEmbeddings(
            model_name="BAAI/bge-small-zh-v1.5",
            model_kwargs={'device': DEVICE}
        )
        
        # 2. è¿æ¥åˆ°ç°æœ‰çš„ Chroma (åªè¯»æ¨¡å¼)
        self.vectorstore = Chroma(
            collection_name=collection_name,
            embedding_function=self.embedding_func,
            persist_directory=persist_path
        )
        
    def query(self, question: str, k=3):
        """
        ç›´æ¥æœå‘é‡åº“ï¼Œä¸æŸ¥ docstore.json
        """
        # è¿”å› Document å¯¹è±¡åˆ—è¡¨
        docs = self.vectorstore.similarity_search(question, k=k)
        # æ‹¼æ¥å†…å®¹
        context_str = "\n\n".join([d.page_content for d in docs])
        return context_str
    
# ==============================================================================
# 1. å®šä¹‰ RAG äº¤äº’é€»è¾‘ (è®©æ³•å¸ˆå‚åŠ è€ƒè¯•)
# ==============================================================================
def call_agent(question, context):
    """
    è°ƒç”¨ä½ çš„â€˜æ…§è¯­â€™æ³•å¸ˆç”Ÿæˆå›ç­”
    """
    # æ¨¡æ‹Ÿæ— å†å²è®°å½•çš„å•è½®é—®ç­”
    response = get_buddhist_master_response(
        question=question, 
        context=context, 
        chat_history=[]
    )
    return response

# ==============================================================================
# 2. æ ¸å¿ƒè¯„ä¼°ä¸»ç¨‹åº
# ==============================================================================
def run_evaluation():
    if not os.path.exists(TESTSET_PATH):
        print(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•é›† {TESTSET_PATH}")
        return

    test_df = pd.read_csv(TESTSET_PATH)
    print(f"--- ğŸ“‚ åŠ è½½æµ‹è¯•é›†æˆåŠŸï¼Œå…± {len(test_df)} é¢˜ ---")

    # âœ… å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨ä¸Šé¢çš„ DirectChromaRetriever æ›¿ä»£ BuddhistRecursiveRetriever
    # è¿™æ ·å°±ä¸ä¼šå»è¯»é‚£ä¸ªä¸å­˜åœ¨çš„ json æ–‡ä»¶äº†
    try:
        retriever = DirectChromaRetriever(persist_path=PERSIST_PATH)
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ src/config.py ä¸­çš„ PERSIST_PATH æ˜¯å¦æŒ‡å‘äº†æ­£ç¡®çš„ chroma_db æ–‡ä»¶å¤¹")
        return

    print("--- ğŸš€ å¼€å§‹åº”è¯•... ---")
    
    answers = []
    contexts = []
    
    for idx, row in test_df.iterrows():
        question = row['user_input']
        
        # 1. æ£€ç´¢ (ä½¿ç”¨çš„æ˜¯å­æ–‡æ¡£åˆ‡ç‰‡ï¼Œè€Œéå®Œæ•´çˆ¶æ–‡æ¡£)
        # è™½ç„¶è¿™ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡å˜çŸ­ï¼Œä½†è¶³å¤Ÿè·‘é€šè¯„ä¼°æµç¨‹
        raw_context = retriever.query(question)
        
        # 2. ç”Ÿæˆ
        answer = call_agent(question, raw_context)
        
        answers.append(answer)
        contexts.append([raw_context])

    ragas_data = {
        'question': test_df['user_input'].tolist(),  # ğŸ‘ˆ æ˜ å°„ user_input -> question
        'answer': answers,
        'contexts': contexts,
        'ground_truth': test_df['reference'].tolist() # ğŸ‘ˆ æ˜ å°„ reference -> ground_truth
    }
    ragas_dataset = Dataset.from_dict(ragas_data)

    # --- è£åˆ¤é…ç½® (ä¿æŒä¸å˜) ---
    print("--- âš–ï¸ é…ç½®ä¸­æ–‡è£åˆ¤... ---")
    openai_client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url="https://api.deepseek.com/v1",
        timeout=120.0
    )

    judge_llm = llm_factory(
        model='deepseek-chat', 
        client=openai_client,
        system_prompt="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è€ƒå®˜ã€‚è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„æ ‡å‡†ç­”æ¡ˆå¯¹å›ç­”è¿›è¡Œæ‰“åˆ†ã€‚æ‰€æœ‰åˆ†æç†ç”±(Reason)å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºã€‚"
    )

    judge_embeddings = LangChainHFEmbeddings(
        model="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': DEVICE}
    )

    metrics = [
        Faithfulness(llm=judge_llm),
        AnswerRelevancy(llm=judge_llm, embeddings=judge_embeddings),
        ContextPrecision(llm=judge_llm),
        ContextRecall(llm=judge_llm)
    ]

    print("--- ğŸ“ å¼€å§‹è¯„åˆ†... ---")
    run_config = RunConfig(max_workers=5, timeout=180, max_retries=3)

    results = evaluate(
        dataset=ragas_dataset,
        metrics=metrics,
        llm=judge_llm, 
        embeddings=judge_embeddings,
        run_config=run_config
    )

    print("\nğŸ† è¯„ä¼°å®Œæˆ ğŸ†")
    results.to_pandas().to_csv(OUTPUT_REPORT, index=False, encoding="utf-8-sig")
    print(f"--- âœ… æŠ¥å‘Šä¿å­˜: {OUTPUT_REPORT} ---")

if __name__ == "__main__":
    run_evaluation()




#! Test ragas.metrics
# import pandas as pd
# from datasets import Dataset
# from ragas import evaluate
# from ragas.metrics import Faithfulness, AnswerRelevancy, ContextPrecision, ContextRecall

# # æ„é€ ä¸€ä¸ªæœ€å°æ•°æ®é›†
# ragas_data = {
#     "question": ["ä½›ç»ä¸­å› æœçš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"],
#     "answer": ["å› æœæŒ‡çš„æ˜¯è¡Œä¸ºä¸ç»“æœçš„å¿…ç„¶è”ç³»ã€‚"],
#     "contexts": [["ä½›ç»åŸæ–‡ç‰‡æ®µï¼šå› æœå¾‹å¼ºè°ƒè¡Œä¸ºå†³å®šç»“æœã€‚"]],
#     "ground_truth": ["ä½›ç»å¼ºè°ƒå› æœå¾‹ï¼Œè¡Œä¸ºå†³å®šç»“æœã€‚"]
# }
# ragas_dataset = Dataset.from_dict(ragas_data)

# # --- è£åˆ¤é…ç½® (ä¿æŒä¸å˜) ---
# print("--- âš–ï¸ é…ç½®ä¸­æ–‡è£åˆ¤... ---")
# openai_client = OpenAI(
#         api_key="sk-9009b81bab1740c9b5dc77c9998148b1",
#         base_url="https://api.deepseek.com/v1"
#         # timeout=60.0, # å¢åŠ è¶…æ—¶æ—¶é—´åˆ° 60 ç§’
#         # max_retries=3  # å¢åŠ è‡ªåŠ¨é‡è¯•æ¬¡æ•°
        
#     )

# judge_llm = llm_factory(
#     model='deepseek-chat', 
#     client=openai_client,
#     system_prompt="ä½ æ˜¯ä¸€ä¸ªå…¬æ­£çš„è€ƒå®˜ã€‚è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„æ ‡å‡†ç­”æ¡ˆå¯¹å›ç­”è¿›è¡Œæ‰“åˆ†ã€‚æ‰€æœ‰åˆ†æç†ç”±(Reason)å¿…é¡»ä½¿ç”¨ç®€ä½“ä¸­æ–‡è¾“å‡ºã€‚"
# )

# judge_embeddings = HuggingFaceEmbeddings(
#     model="BAAI/bge-small-zh-v1.5"
#     # model_kwargs={'device': DEVICE}
# )

# metrics = [
#     Faithfulness(llm=judge_llm),
#     AnswerRelevancy(llm=judge_llm, embeddings=judge_embeddings),
#     ContextPrecision(llm=judge_llm),
#     ContextRecall(llm=judge_llm)
# ]

# # æ‰§è¡Œè¯„ä¼°
# results = evaluate(
#     dataset=ragas_dataset,
#     metrics=metrics
# )

# # è¾“å‡ºç»“æœ
# df = results.to_pandas()
# print(df)
# df.to_csv("evaluation_report.csv", index=False, encoding="utf-8-sig")
# print("--- âœ… æŠ¥å‘Šå·²ä¿å­˜: evaluation_report.csv ---")
